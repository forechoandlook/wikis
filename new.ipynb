{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new",
      "provenance": [],
      "collapsed_sections": [
        "R6fTVmp3KVpj"
      ],
      "authorship_tag": "ABX9TyNo20NupRCMnrVE3YAstPxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forechoandlook/wikis/blob/master/new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data prepare "
      ],
      "metadata": {
        "id": "R6fTVmp3KVpj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4SJAu3jKDV-"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/sh/zybmnhahvhka7wt/AACgm5NeTOAOzBdM0wHjSvGfa\n",
        "!unzip AACgm5NeTOAOzBdM0wHjSvGfa\n",
        "!rm -rf AACgm5NeTOAOzBdM0wHjSvGfa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_zwy"
      ],
      "metadata": {
        "id": "_1a2bgEvKMQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## codes"
      ],
      "metadata": {
        "id": "-wJsjd8CKaDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the training, validation and testing dataset\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "dataPath  = './'\n",
        "\n",
        "trainPath =  dataPath \n",
        "testPath  =  dataPath\n",
        "valPath   =  dataPath \n",
        "\n",
        "trainImgsFile   = 'trainingDataImgs.npy'\n",
        "trainLabelsFile = 'trainingDataLabels.npy'\n",
        "\n",
        "testImgsFile    = 'testingDataImgs.npy'\n",
        "testLabelsFile  = 'testingDataLabels.npy'\n",
        "\n",
        "valImgsFile     = 'validationDataImgs.npy'\n",
        "valLabelsFile   = 'validationDataLabels.npy'\n",
        "\n",
        "def getData(path,fileName):\n",
        "  return np.load(path + \"/\" + fileName)\n",
        "\n",
        "trainImgs = getData(trainPath, trainImgsFile)\n",
        "trainLabels = getData(trainPath, trainLabelsFile)\n",
        "\n",
        "testImgs   = getData(testPath, testImgsFile)\n",
        "testLabels = getData(testPath, testLabelsFile)\n",
        "\n",
        "valImgs    = getData(valPath, valImgsFile)\n",
        "valLabels  = getData(valPath, valLabelsFile)\n",
        "\n",
        "print(\"Got train, test and validation data\")\n",
        "print(trainImgs.shape, testImgs.shape, valImgs.shape)\n",
        "print(trainLabels.shape, testLabels.shape, valLabels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxwj2NGoKM6b",
        "outputId": "fa6fa415-1544-46ae-8476-ed823a9bc98b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got train, test and validation data\n",
            "(1000, 480, 320, 1) (617, 480, 320, 1) (251, 480, 320, 1)\n",
            "(1000, 480, 320, 1) (617, 480, 320, 1) (251, 480, 320, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm  \n",
        "from tensorflow import keras\n",
        "# from segmentation_models import Backbones\n",
        "from segmentation_models.models.unet import *\n",
        "sm.set_framework('tf.keras')\n",
        "\n",
        "sm.framework()\n",
        "\n",
        "\n",
        "kwargs = {\n",
        "    'backend': keras.backend,\n",
        "    'layers': keras.layers,\n",
        "    'models': keras.models,\n",
        "    'utils': keras.utils,\n",
        "}\n",
        "\n",
        "backend, layers, models, keras_utils  = kwargs['backend'], kwargs['layers'], kwargs['models'], kwargs['utils']\n",
        "\n",
        "set_kwargs(kwargs)\n",
        "\n",
        "import cv2\n",
        "\n",
        "def random_crop(image,label,crop=0.8):\n",
        "    # \n",
        "    img_sz = image.shape\n",
        "    \n",
        "    crop_sz = [int(img_sz[0]*crop),int(img_sz[1]*crop)]\n",
        "    y,x = (np.random.randint(img_sz[0]-crop_sz[0]+1),np.random.randint(img_sz[1]-crop_sz[1]+1))\n",
        "    h,w = int(img_sz[0]*crop),int(img_sz[1]*crop)\n",
        "    image_crop = image[y:y+h,x:x+w,:]\n",
        "    label_crop = label[y:y+h,x:x+w,:]\n",
        "    img,lab = adaptive_resize(image_crop,label_crop,img_sz[:2][::-1])\n",
        "    return img,lab\n",
        "\n",
        "def adaptive_resize(image,label,target_size):\n",
        "    new_image = cv2.resize(image,target_size)\n",
        "    new_label=cv2.resize(label,target_size)\n",
        "    return new_image, new_label\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, datas, labels,argument=True, batch_size=32, shuffle=True):\n",
        "        'Initialization'\n",
        "        super().__init__()\n",
        "        self.data = datas\n",
        "        self.mask = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list(range(datas.shape[0]))\n",
        "        self.shuffle = shuffle\n",
        "        self.argument = argument\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, Y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        \n",
        "        res_x = []\n",
        "        res_y = []\n",
        "        for i in list_IDs_temp:\n",
        "            if np.random.rand() < 0.5 and self.argument:\n",
        "\n",
        "                img,label = random_crop(self.data[i],self.mask[i])\n",
        "                img = np.expand_dims(img,-1)\n",
        "                label = np.expand_dims(label,-1)\n",
        "                res_x.append(np.expand_dims(img,0))\n",
        "                res_y.append(np.expand_dims(label,0))\n",
        "            else:\n",
        "                res_x.append(np.expand_dims(self.data[i],0))\n",
        "                res_y.append(np.expand_dims(self.mask[i],0))\n",
        "    \n",
        "        X = np.concatenate(res_x,axis=0)\n",
        "        Y = np.concatenate(res_y,axis=0)\n",
        "        return X,Y\n",
        "\n",
        "def focal_loss_my(y_true,y_pred):\n",
        "    print(y_true.shape,y_pred.shape)\n",
        "    ce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    y_true = tf.cast(y_true,tf.float32)\n",
        "    p_t = tf.math.exp(-ce_loss)\n",
        "    focal_loss = tf.reduce_mean((1-p_t)**2*ce_loss)\n",
        "    return focal_loss\n",
        "\n",
        "def weighted_sparse_cross_entropy_wrapper(weight=[],classes=12):\n",
        "    class_weight = tf.constant([1]+[10]*11,dtype=tf.float32)\n",
        "    eposion = tf.constant(1e-7,dtype=tf.float32)\n",
        "  # adapetive loss \n",
        "    def sparse_cross_entropy(y_true,y_pred):\n",
        "        res = tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred, weight)\n",
        "        return res\n",
        "    \n",
        "    return sparse_cross_entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrhxUd3vKM8y",
        "outputId": "88293bed-faf1-40ef-eccd-d74b7f36ecd4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Models: using `keras` framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## net"
      ],
      "metadata": {
        "id": "k8X3yVIpKvH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model (without pre-trained ImageNet weights)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 60\n",
        "# EPOCHS     = 20 (use early stopping as well to avoid overfitting (and store the epochs till then))\n",
        "import os\n",
        "import numpy as np\n",
        "# import segmentation_models as sm\n",
        "\n",
        "def upsampleN(target_filters=16, stage=1, use_batchnorm=True):\n",
        "\n",
        "    t =2 ** (stage - 1) \n",
        "    up_name = 'attention_decoder_stage{}_upsampling'.format(t)\n",
        "    conv1_name = 'attention_decoder_stage{}_a'.format(t)\n",
        "\n",
        "    def wrapper(input_tensor):\n",
        "\n",
        "        if t == 0:\n",
        "            return input_tensor\n",
        "        x = input_tensor \n",
        "        \n",
        "        x = layers.UpSampling2D(size=t, name=up_name)(x)\n",
        "\n",
        "        x = Conv3x3BnReLU(target_filters, use_batchnorm, name=conv1_name)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def Conv1x1BnReLU(filters, use_batchnorm, name):\n",
        "\n",
        "    def wrapper(input_tensor):\n",
        "\n",
        "        conv = layers.Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=(1, 1),\n",
        "            padding='same',\n",
        "            use_bias=not (use_batchnorm),\n",
        "            kernel_initializer='glorot_uniform',\n",
        "            name=name,\n",
        "        )(input_tensor)\n",
        "        if use_batchnorm:\n",
        "            conv = layers.BatchNormalization(name=name + '_bn')(conv)\n",
        "        # conv = layers.ReLU(name=name + '_relu')(conv)\n",
        "        return conv\n",
        "    return wrapper\n",
        "\n",
        "def ConvUpsample(target_filters=16, stage=1, use_batchnorm=False, name=\"\"):\n",
        "\n",
        "    t =2 ** (stage - 1) \n",
        "    up_name = 'attention_decoder_stage{}{}_upsampling'.format(t,name)\n",
        "    conv1_name = 'attention_decoder_stage{}{}_a'.format(t,name)\n",
        "\n",
        "    def wrapper(input_tensor):\n",
        "\n",
        "        if t == 0:\n",
        "            return input_tensor\n",
        "        x = input_tensor \n",
        "        \n",
        "        # x = Conv3x3BnReLU(target_filters, use_batchnorm, name=conv1_name)(x)\n",
        "        \n",
        "        x = Conv1x1BnReLU(target_filters, use_batchnorm, name=conv1_name)(x)\n",
        "        \n",
        "        x = layers.UpSampling2D(size=t, name=up_name)(x)\n",
        "\n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def multiply_with_transpose(a,b):\n",
        "      return tf.transpose(tf.transpose(a) * b)\n",
        "\n",
        "def pyramidAtt(x, hidden=6):\n",
        "    # x : [x0, x1, x2, x3, x4, x5,]\n",
        "    # x0 : [batch_size, height, width, 1]\n",
        "    b, h, w, c = x[1].shape\n",
        "    # conv21 = Conv1x1BnReLU(filters=1, use_batchnorm=True, name=\"att-21-\"+str(i))\n",
        "    pool = keras.layers.AvgPool2D(pool_size=(7,7),strides=3)\n",
        "    linear_layer = keras.layers.Dense(hidden, use_bias=True, activation='relu')\n",
        "    get_attention= keras.layers.Dense(6, use_bias=True, activation='relu')\n",
        "    res = []\n",
        "    for i in range(len(x)):\n",
        "        # mean channels into 1\n",
        "        x_mean = tf.reduce_mean(x[i], axis=-1, keepdims=True)\n",
        "        x_mean = pool(x_mean)\n",
        "        x_reshape = keras.layers.Flatten()(x_mean)\n",
        "        xi = linear_layer(x_reshape)\n",
        "        res.append(xi)\n",
        "    # concat res into [batch_size, hidden*5]\n",
        "    res = tf.concat(res, axis=1)\n",
        "    # get attention\n",
        "    res = get_attention(res)\n",
        "    # res : [batch_size, 5]\n",
        "    # softmax\n",
        "    res = tf.nn.softmax(res)\n",
        "    \n",
        "    res = [multiply_with_transpose(xi,res[:,i]) for i, xi in enumerate(x)]\n",
        "    \n",
        "    return res\n",
        "\n",
        "def build_unet_my(\n",
        "        backbone,\n",
        "        decoder_block,\n",
        "        skip_connection_layers,\n",
        "        decoder_filters=(256, 128, 64, 32, 16),\n",
        "        n_upsample_blocks=5,\n",
        "        classes=1,\n",
        "        activation='sigmoid',\n",
        "        use_batchnorm=True,\n",
        "):\n",
        "    input_ = backbone.input\n",
        "    x = backbone.output\n",
        "\n",
        "    # extract skip connections\n",
        "    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n",
        "              else backbone.get_layer(index=i).output for i in skip_connection_layers])\n",
        "\n",
        "    # add center block if previous operation was maxpooling (for vgg models)\n",
        "    if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n",
        "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block1')(x)\n",
        "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block2')(x)\n",
        "\n",
        "    # building decoder blocks\n",
        "    last_feature = [ConvUpsample(stage=2,use_batchnorm=False,name=\"encoder\")(skips[-1])]\n",
        "    for i in range(n_upsample_blocks):\n",
        "\n",
        "        if i < len(skips):\n",
        "            skip = skips[i]\n",
        "        else:\n",
        "            skip = None\n",
        "\n",
        "        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm)(x, skip)\n",
        "        last_feature.append(ConvUpsample(stage=n_upsample_blocks-i,use_batchnorm=False)(x))\n",
        "\n",
        "    # last feature upsample to initial image size and aggreate \n",
        "    last_feature = pyramidAtt(last_feature)\n",
        "    \n",
        "    res = 0\n",
        "    for i in last_feature:\n",
        "        res += i\n",
        "    \n",
        "    # last_feature = layers.Concatenate(axis=-1)(last_feature)\n",
        "    \n",
        "    \n",
        "    x = Conv3x3BnReLU(decoder_filters[-1]*1, use_batchnorm, name='last_feature_conv')(res)\n",
        "    # x = Conv3x3BnReLU(decoder_filters[-1]*1, use_batchnorm, name='last_feature_conv_2')(x)\n",
        "    # model head (define number of output classes)\n",
        "    x = layers.Conv2D(\n",
        "        filters=classes,\n",
        "        kernel_size=(3, 3),\n",
        "        padding='same',\n",
        "        use_bias=True,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        name='final_conv',\n",
        "    )(x)\n",
        "    x = layers.Activation(activation, name=activation)(x)\n",
        "\n",
        "    # create keras model instance\n",
        "    model = models.Model(input_, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def MyUnetOut(\n",
        "        backbone_name='vgg16',\n",
        "        input_shape=(None, None, 3),\n",
        "        classes=1,\n",
        "        activation='sigmoid',\n",
        "        weights=None,\n",
        "        encoder_weights='imagenet',\n",
        "        encoder_freeze=False,\n",
        "        use_attention = False,\n",
        "        encoder_features='default',\n",
        "        decoder_block_type='upsampling',\n",
        "        decoder_filters=(256, 128, 64, 32, 16),\n",
        "        decoder_use_batchnorm=True,\n",
        "):\n",
        "\n",
        "    backend, layers, models, keras_utils  = kwargs['backend'], kwargs['layers'], kwargs['models'], kwargs['utils']\n",
        "    if decoder_block_type == 'upsampling':\n",
        "        decoder_block = DecoderUpsamplingX2Block\n",
        "    elif decoder_block_type == 'transpose':\n",
        "        decoder_block = DecoderTransposeX2Block\n",
        "    else:\n",
        "        raise ValueError('Decoder block type should be in (\"upsampling\", \"transpose\"). '\n",
        "                         'Got: {}'.format(decoder_block_type))\n",
        "\n",
        "    backbone = Backbones.get_backbone(\n",
        "        backbone_name,\n",
        "        input_shape=input_shape,\n",
        "        weights=encoder_weights,\n",
        "        include_top=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "    if encoder_features == 'default':\n",
        "        encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n",
        "    \n",
        "    if not use_attention:\n",
        "        model = build_unet(\n",
        "            backbone=backbone,\n",
        "            decoder_block=decoder_block,\n",
        "            skip_connection_layers=encoder_features,\n",
        "            decoder_filters=decoder_filters,\n",
        "            classes=classes,\n",
        "            activation=activation,\n",
        "            n_upsample_blocks=len(decoder_filters),\n",
        "            use_batchnorm=decoder_use_batchnorm,\n",
        "        )\n",
        "    else:\n",
        "        model = build_unet_my(\n",
        "            backbone=backbone,\n",
        "            decoder_block=decoder_block,\n",
        "            skip_connection_layers=encoder_features,\n",
        "            decoder_filters=decoder_filters,\n",
        "            classes=classes,\n",
        "            activation=activation,\n",
        "            n_upsample_blocks=len(decoder_filters),\n",
        "            use_batchnorm=decoder_use_batchnorm,\n",
        "        )\n",
        "    \n",
        "    # lock encoder weights for fine-tuning\n",
        "    if encoder_freeze:\n",
        "        freeze_model(backbone, **kwargs)\n",
        "\n",
        "    # loading model weights\n",
        "    if weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4kCK69ZNKM_2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyUnetOut(\"resnet34\",input_shape=(480, 320, 1),use_attention=True, encoder_weights=None, classes=12, activation='softmax')"
      ],
      "metadata": {
        "id": "M6BZ63ZeKNB8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "each_class = ['background',\n",
        " 'bottle',\n",
        " 'can',\n",
        " 'chain',\n",
        " 'drink-carton',\n",
        " 'hook',\n",
        " 'propeller',\n",
        " 'shampoo-bottle',\n",
        " 'standing-bottle',\n",
        " 'tire',\n",
        " 'valve',\n",
        " 'wall']\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\n",
        "  def __init__(self,\n",
        "               y_true=None,\n",
        "               y_pred=None,\n",
        "               num_classes=None,\n",
        "               name=None,\n",
        "               dtype=None,\n",
        "              each_class=None):\n",
        "    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
        "    self.each_class = each_class\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "    return super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "LOSS=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "model.compile(\n",
        "    'Adam',\n",
        "    #loss = LOSS,\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "#     loss=weighted_sparse_cross_entropy_wrapper(),\n",
        "    metrics=[UpdatedMeanIoU(num_classes=12,each_class=each_class)],\n",
        ")"
      ],
      "metadata": {
        "id": "2V6O5AFIKNEn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model.fit(trainImgs, trainLabels, epochs=60, batch_size=16, validation_data=(testImgs, testLabels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgSz2qqxKNHo",
        "outputId": "37e299c7-bbee-4f83-f32f-02014e4880e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "63/63 [==============================] - 148s 2s/step - loss: 1.1658 - updated_mean_io_u: 0.0690 - val_loss: 642059.1250 - val_updated_mean_io_u: 0.0803\n",
            "Epoch 2/60\n",
            "63/63 [==============================] - 98s 2s/step - loss: 0.1597 - updated_mean_io_u: 0.1103 - val_loss: 5.7155 - val_updated_mean_io_u: 0.0802\n",
            "Epoch 3/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.1166 - updated_mean_io_u: 0.1416 - val_loss: 0.3122 - val_updated_mean_io_u: 0.0980\n",
            "Epoch 4/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0968 - updated_mean_io_u: 0.1758 - val_loss: 0.2791 - val_updated_mean_io_u: 0.1383\n",
            "Epoch 5/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0790 - updated_mean_io_u: 0.2259 - val_loss: 0.0973 - val_updated_mean_io_u: 0.2058\n",
            "Epoch 6/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0718 - updated_mean_io_u: 0.2559 - val_loss: 0.9568 - val_updated_mean_io_u: 0.1187\n",
            "Epoch 7/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0592 - updated_mean_io_u: 0.2912 - val_loss: 0.5442 - val_updated_mean_io_u: 0.1821\n",
            "Epoch 8/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0527 - updated_mean_io_u: 0.3566 - val_loss: 0.1021 - val_updated_mean_io_u: 0.2076\n",
            "Epoch 9/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0482 - updated_mean_io_u: 0.3889 - val_loss: 0.0636 - val_updated_mean_io_u: 0.3033\n",
            "Epoch 10/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0473 - updated_mean_io_u: 0.4011 - val_loss: 0.1102 - val_updated_mean_io_u: 0.2516\n",
            "Epoch 11/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0472 - updated_mean_io_u: 0.4152 - val_loss: 0.0458 - val_updated_mean_io_u: 0.3984\n",
            "Epoch 12/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0515 - updated_mean_io_u: 0.4130 - val_loss: 0.1055 - val_updated_mean_io_u: 0.2383\n",
            "Epoch 13/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0387 - updated_mean_io_u: 0.5152 - val_loss: 0.0443 - val_updated_mean_io_u: 0.4118\n",
            "Epoch 14/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0341 - updated_mean_io_u: 0.5716 - val_loss: 0.0385 - val_updated_mean_io_u: 0.4874\n",
            "Epoch 15/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0311 - updated_mean_io_u: 0.6017 - val_loss: 0.0373 - val_updated_mean_io_u: 0.5749\n",
            "Epoch 16/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0322 - updated_mean_io_u: 0.6136 - val_loss: 0.1601 - val_updated_mean_io_u: 0.2308\n",
            "Epoch 17/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0339 - updated_mean_io_u: 0.6251 - val_loss: 0.0378 - val_updated_mean_io_u: 0.5774\n",
            "Epoch 18/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0291 - updated_mean_io_u: 0.6805 - val_loss: 0.0407 - val_updated_mean_io_u: 0.5344\n",
            "Epoch 19/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0276 - updated_mean_io_u: 0.6816 - val_loss: 0.0298 - val_updated_mean_io_u: 0.6537\n",
            "Epoch 20/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0265 - updated_mean_io_u: 0.6964 - val_loss: 0.0333 - val_updated_mean_io_u: 0.6285\n",
            "Epoch 21/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0256 - updated_mean_io_u: 0.7052 - val_loss: 0.0305 - val_updated_mean_io_u: 0.6616\n",
            "Epoch 22/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0256 - updated_mean_io_u: 0.7022 - val_loss: 0.0360 - val_updated_mean_io_u: 0.5843\n",
            "Epoch 23/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0267 - updated_mean_io_u: 0.7010 - val_loss: 0.0310 - val_updated_mean_io_u: 0.6667\n",
            "Epoch 24/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0253 - updated_mean_io_u: 0.7094 - val_loss: 0.0309 - val_updated_mean_io_u: 0.6586\n",
            "Epoch 25/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0291 - updated_mean_io_u: 0.6870 - val_loss: 0.0741 - val_updated_mean_io_u: 0.4419\n",
            "Epoch 26/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0264 - updated_mean_io_u: 0.7031 - val_loss: 0.0325 - val_updated_mean_io_u: 0.6648\n",
            "Epoch 27/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0234 - updated_mean_io_u: 0.7254 - val_loss: 0.0295 - val_updated_mean_io_u: 0.6567\n",
            "Epoch 28/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0229 - updated_mean_io_u: 0.7317 - val_loss: 0.0296 - val_updated_mean_io_u: 0.6705\n",
            "Epoch 29/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0222 - updated_mean_io_u: 0.7367 - val_loss: 0.0292 - val_updated_mean_io_u: 0.6782\n",
            "Epoch 30/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0221 - updated_mean_io_u: 0.7370 - val_loss: 0.0326 - val_updated_mean_io_u: 0.6027\n",
            "Epoch 31/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0227 - updated_mean_io_u: 0.7287 - val_loss: 0.0312 - val_updated_mean_io_u: 0.6624\n",
            "Epoch 32/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0204 - updated_mean_io_u: 0.7532 - val_loss: 0.0313 - val_updated_mean_io_u: 0.6651\n",
            "Epoch 33/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0207 - updated_mean_io_u: 0.7488 - val_loss: 0.0283 - val_updated_mean_io_u: 0.7006\n",
            "Epoch 34/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0196 - updated_mean_io_u: 0.7545 - val_loss: 0.0348 - val_updated_mean_io_u: 0.6597\n",
            "Epoch 35/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0207 - updated_mean_io_u: 0.7483 - val_loss: 0.0723 - val_updated_mean_io_u: 0.5160\n",
            "Epoch 36/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0221 - updated_mean_io_u: 0.7342 - val_loss: 0.0410 - val_updated_mean_io_u: 0.5923\n",
            "Epoch 37/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0201 - updated_mean_io_u: 0.7539 - val_loss: 0.0294 - val_updated_mean_io_u: 0.7115\n",
            "Epoch 38/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0200 - updated_mean_io_u: 0.7584 - val_loss: 0.0434 - val_updated_mean_io_u: 0.5199\n",
            "Epoch 39/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0188 - updated_mean_io_u: 0.7655 - val_loss: 0.0340 - val_updated_mean_io_u: 0.6757\n",
            "Epoch 40/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0191 - updated_mean_io_u: 0.7608 - val_loss: 0.0326 - val_updated_mean_io_u: 0.6589\n",
            "Epoch 41/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0175 - updated_mean_io_u: 0.7759 - val_loss: 0.0325 - val_updated_mean_io_u: 0.6669\n",
            "Epoch 42/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0186 - updated_mean_io_u: 0.7620 - val_loss: 0.0304 - val_updated_mean_io_u: 0.6779\n",
            "Epoch 43/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0187 - updated_mean_io_u: 0.7681 - val_loss: 0.0508 - val_updated_mean_io_u: 0.6210\n",
            "Epoch 44/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0196 - updated_mean_io_u: 0.7594 - val_loss: 0.0554 - val_updated_mean_io_u: 0.5549\n",
            "Epoch 45/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0173 - updated_mean_io_u: 0.7766 - val_loss: 0.0352 - val_updated_mean_io_u: 0.6562\n",
            "Epoch 46/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0173 - updated_mean_io_u: 0.7831 - val_loss: 0.0434 - val_updated_mean_io_u: 0.5846\n",
            "Epoch 47/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0178 - updated_mean_io_u: 0.7756 - val_loss: 0.0319 - val_updated_mean_io_u: 0.6906\n",
            "Epoch 48/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0167 - updated_mean_io_u: 0.7896 - val_loss: 0.0322 - val_updated_mean_io_u: 0.6963\n",
            "Epoch 49/60\n",
            "63/63 [==============================] - 101s 2s/step - loss: 0.0317 - updated_mean_io_u: 0.6640 - val_loss: 0.1518 - val_updated_mean_io_u: 0.2899\n",
            "Epoch 50/60\n",
            "63/63 [==============================] - 100s 2s/step - loss: 0.0207 - updated_mean_io_u: 0.7483 - val_loss: 0.0417 - val_updated_mean_io_u: 0.5636\n",
            "Epoch 51/60\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0211 - updated_mean_io_u: 0.7460"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K9j0Zz9xLpdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p-lnA3JaLqGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SvSR-PleLqJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EeM9yrO8LqNI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}